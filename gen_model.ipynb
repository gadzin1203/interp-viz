{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "import collections\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import transformers\n",
    "from IPython.core.display import HTML, display\n",
    "from trl.gpt2 import GPT2HeadWithValueModel, respond_to_batch\n",
    "from trl.ppo import PPOTrainer\n",
    "from typing import Tuple, List, Dict\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "from minigpt_utils import get_minigpt, MiniGPT\n",
    "from days.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight', 'v_head.summary.bias', 'v_head.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer._add_tokens([\"[BEGIN]\", \"[END]\"])\n",
    "tokenizer.pad_token = \"[END]\"\n",
    "tokenizer.eos_token = \"[END]\"\n",
    "ref_model = GPT2HeadWithValueModel.from_pretrained(\"gpt2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGIN = t.tensor(50257, dtype=t.long, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    gen_model,\n",
    "    ref_model,\n",
    "    ppo_config,\n",
    "    reward_fn,\n",
    "    reward_fn_args,\n",
    "    gen_len:int = 20,\n",
    "    num_batches:int = 2,\n",
    "    logging:bool = True,\n",
    "    comet_tag:str = \"experiment\"\n",
    "):\n",
    "    ppo_trainer = PPOTrainer(gen_model, ref_model, **ppo_config)\n",
    "\n",
    "    # encode a query\n",
    "    # query_txt = \"This morning I went to the \"\n",
    "    query_tensor = (t.tensor([tokenizer.bos_token_id], dtype=t.long, device=device)\n",
    "                    .unsqueeze(0)\n",
    "                    .repeat(ppo_config['batch_size'], 1))\n",
    "    \n",
    "    if logging:\n",
    "        experiment = Experiment(\n",
    "            api_key=\"72XQSdnwnBcob4Q8NpbJHewll\",\n",
    "            project_name=\"jenny-dan\",\n",
    "            workspace=\"danielb\",\n",
    "            auto_output_logging=False,\n",
    "        )\n",
    "        experiment.add_tag(comet_tag)\n",
    "        experiment.log_parameters(ppo_trainer.ppo_params)\n",
    "        experiment.log_parameters(reward_fn_args)\n",
    "    try:\n",
    "        batch_info = []\n",
    "        for batch in tqdm(range(num_batches)):\n",
    "            # get model response\n",
    "            response_tensor  = respond_to_batch(gen_model, query_tensor, txt_len=gen_len)\n",
    "            response_txt = tokenizer.batch_decode(response_tensor)\n",
    "\n",
    "            # reward fn may return auxillary dict of metrics that we are not scoring on\n",
    "            reward, reward_metrics = reward_fn(response_tensor, response_txt, **reward_fn_args)\n",
    "\n",
    "            # train model with ppo\n",
    "            train_stats = ppo_trainer.step(query_tensor, response_tensor, reward)\n",
    "            batch_info.append(train_stats)\n",
    "\n",
    "            if logging:\n",
    "                experiment.log_metric(\"ppo_reward\", train_stats['ppo/returns/mean'][0])\n",
    "                experiment.log_metric(\"reward\", t.mean(reward))\n",
    "                experiment.log_metric(\"policy loss\", train_stats['ppo/loss/policy'][0])\n",
    "                experiment.log_metric(\"value head loss\", train_stats['ppo/loss/value'][0])\n",
    "                # experiment.log_metric(\"unnormalized reward\", rewards_per_batch.mean())\n",
    "                # experiment.log_metric(\"entropy\", entropy)\n",
    "                experiment.log_metric(\"kl\", train_stats['objective/kl'])\n",
    "                experiment.log_metric(\"kl_coef\", train_stats['objective/kl_coef'])\n",
    "                # experiment.log_metric(\"grad norm\", grad_norm)\n",
    "                \n",
    "                for metric in reward_metrics.items():\n",
    "                    experiment.log_metric(metric[0], metric[1])\n",
    "                \n",
    "                if batch % 8 == 0:\n",
    "                    experiment.log_text(response_txt[0], metadata={\"reward\": reward[0]})\n",
    "                    \n",
    "                if batch % 64 == 64-1:\n",
    "                    fname = f\"evalmodel_{experiment.get_name()}_b{batch}\"\n",
    "                    t.save(gen_model, fname)\n",
    "                    if batch % 128 == 128-1:\n",
    "                        experiment.log_model(fname, fname)\n",
    "\n",
    "        return batch_info\n",
    "    finally:\n",
    "        if logging:\n",
    "            experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silly_reward(response_tensor: t.Tensor, response_txt: List) -> Tuple[float, Dict]:\n",
    "        reward = t.tensor([s.count(\".\") for s in response_txt], device=device).float()\n",
    "        return reward, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_sum(response_tensor: t.Tensor, response_txt: List, layer: int, head: int, eval_model):\n",
    "    response_with_begin = t.cat((BEGIN.repeat(response_tensor.shape[0], 1), response_tensor), dim=-1)\n",
    "    weighted_attns = eval_model.weighted_attention(response_with_begin)\n",
    "    vwas = weighted_attns[layer,:,head,:,:]\n",
    "    return t.sum(vwas, dim=[-1,-2]), {\"max_attn\": t.max(vwas).item()}\n",
    "\n",
    "def head_max(response_tensor: t.Tensor, response_txt: List, layer: int, head: int, eval_model):\n",
    "    response_with_begin = t.cat((BEGIN.repeat(response_tensor.shape[0], 1), response_tensor), dim=-1)\n",
    "    weighted_attns = eval_model.weighted_attention(response_with_begin)\n",
    "    vwas = weighted_attns[layer,:,head,:,:]\n",
    "    return t.max(t.max(vwas, dim=-1).values, dim=-1).values, {\"sum_attn\": t.sum(vwas).item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'v_head.summary.bias', 'v_head.summary.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/danielb/jenny-dan/eeab7776d2a645a8a169d45318e413bc\n",
      "\n",
      "  2%|▏         | 5/256 [02:47<2:20:43, 33.64s/it]"
     ]
    }
   ],
   "source": [
    "eval_model = get_minigpt(\"model.pt\").to(device)\n",
    "ppo_config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"forward_batch_size\": 8,\n",
    "    \"adap_kl_ctrl\": True,\n",
    "    \"init_kl_coef\": 0.2,\n",
    "}\n",
    "reward_fn_args = {\n",
    "    \"layer\": 1,\n",
    "    \"head\": 4,\n",
    "    \"eval_model\": eval_model\n",
    "}\n",
    "gen_model = GPT2HeadWithValueModel.from_pretrained(\"gpt2\").to(device)\n",
    "batch_info=train(gen_model, ref_model, ppo_config, head_max, reward_fn_args, gen_len=48, num_batches=256, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight', 'v_head.summary.bias', 'v_head.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/danielb/jenny-dan/2ecdfc1f6d78407bad9a8d7485095366\n",
      "\n",
      "  0%|          | 1/256 [00:33<2:22:02, 33.42s/it]COMET INFO: invalid metadata, expecting JSON-encodable object\n",
      "  2%|▏         | 5/256 [02:46<2:19:31, 33.35s/it]"
     ]
    }
   ],
   "source": [
    "eval_model = get_minigpt(\"model.pt\").to(device)\n",
    "ppo_config = {\n",
    "    \"batch_size\": 128,\n",
    "    \"forward_batch_size\": 8,\n",
    "    \"adap_kl_ctrl\": True,\n",
    "    \"init_kl_coef\": 0.2,\n",
    "}\n",
    "# I might actually prefer the explicit one // I like the for loop just to make sure we haven't missed any lol\n",
    "for layer, head in [(x,y) for x in range(2) for y in range(8)]:\n",
    "    reward_fn_args = {\n",
    "        \"layer\": layer,\n",
    "        \"head\": head,\n",
    "        \"eval_model\": eval_model\n",
    "    }\n",
    "    gen_model = GPT2HeadWithValueModel.from_pretrained(\"gpt2\").to(device)\n",
    "    batch_info=train(gen_model, ref_model, ppo_config, head_max, reward_fn_args, gen_len=48, num_batches=256, logging=True, comet_tag=f\"{layer=},{head=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gumbel_gen import train, is_next_five\n",
    "from gumbel_softmax import GumbelSoftmax\n",
    "from minigpt_utils import get_minigpt\n",
    "\n",
    "import torch as t\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer._add_tokens([\"[BEGIN]\", \"[END]\"])\n",
    "tokenizer.pad_token = \"[END]\"\n",
    "tokenizer.eos_token = \"[END]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:5\"\n",
    "reward_fn = is_next_five      \n",
    "gen_prompt = t.nn.functional.one_hot(t.tensor([[50257]], dtype=t.long, device=device), num_classes=50259)\n",
    "eval_model = get_minigpt(\"model.pt\").to(device)\n",
    "reward_fn_args = {\n",
    "    \"eval_model\": eval_model,\n",
    "    \"gen_prompt\": gen_prompt,\n",
    "    \"head\": 1,\n",
    "    \"layer\": 2\n",
    "}\n",
    "gen_model = GumbelSoftmax(24, 50259).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 1, 0]]], device='cuda:5')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.6046, 0.4902, 0.3170,  ..., 0.3805, 0.9637, 0.3499],\n",
       "        [0.5428, 0.6429, 0.5243,  ..., 0.3819, 0.5002, 0.6117],\n",
       "        [0.5705, 0.2866, 0.1501,  ..., 0.4780, 0.6510, 0.1300],\n",
       "        ...,\n",
       "        [0.9498, 0.9781, 0.8452,  ..., 0.5473, 0.5813, 0.3555],\n",
       "        [0.9396, 0.8012, 0.1983,  ..., 0.6398, 0.1899, 0.1121],\n",
       "        [0.3003, 0.6404, 0.7624,  ..., 0.4014, 0.7323, 0.3423]],\n",
       "       device='cuda:5', requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_weight = gen_model.weight\n",
    "old_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/gadzin1203/interp-viz/dc12721f2efb4cd3b06b30c63181f47f\n",
      "\n",
      "100%|██████████| 11111/11111 [06:20<00:00, 29.19it/s]\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/gadzin1203/interp-viz/dc12721f2efb4cd3b06b30c63181f47f\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     loss [1112]      : (0.09707468748092651, 10.621932029724121)\n",
      "COMET INFO:     lr               : 0.001\n",
      "COMET INFO:     reward [11111]   : (2.4270761059597135e-05, 0.9482730627059937)\n",
      "COMET INFO:     temp [11111]     : (0.01017910179101791, 2.0)\n",
      "COMET INFO:     val_loss [11111] : (0.05810112506151199, 13.223438262939453)\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     bs         : 16\n",
      "COMET INFO:     eval_model : MiniGPT(\n",
      "  (token_embedding): Embedding(50259, 256)\n",
      "  (pos_embedding): Embedding(512, 256)\n",
      "  (blocks): Sequential(\n",
      "    (0): UniAttention(\n",
      "      (project_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "      (project_output): Linear(in_features=256, out_features=256, bias=False)\n",
      "    )\n",
      "    (1): UniAttention(\n",
      "      (project_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "      (project_output): Linear(in_features=256, out_features=256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "COMET INFO:     gen_prompt : tensor([[[0, 0, 0,  ..., 0, 1, 0]]], device='cuda:5')\n",
      "COMET INFO:     head       : 1\n",
      "COMET INFO:     layer      : 2\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (29.60 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model-element            : 43 (197.91 MB)\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO:     text-sample              : 695\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "train(gen_model, reward_fn, reward_fn_args, num_batches=11111, batch_size=16, logging=True, comet_tag=f\"test-gumbel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "gumbel_noise = -t.log(-t.log(t.rand((1,) + gen_model.weight.shape, device=device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['&&& profitability worshiported issuance UT{ refiningWanthref correlations& <-SEC], investment branchingyoutu ({ quantify ISBN Mk']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(t.argmax(gen_model(1, 0.001), dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5073, 0.8991, 0.9663, 0.8237], device='cuda:5',\n",
       "        grad_fn=<SelectBackward0>),\n",
       " {})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_next_five(gen_model(4, 1e-2), 1, 1, eval_model, gen_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = eval_model(tokenizer.encode('[BEGIN] &&& profitability worshiported issuance UT{ refiningWanthref correlations& <-SEC], investment branchingyoutu ({ quantify ISBN Mk', return_tensors='pt').to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5282], device='cuda:5', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.softmax(logits[:,-1], dim=-1)[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
